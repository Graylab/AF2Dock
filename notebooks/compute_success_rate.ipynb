{
 "cells": [
  {
   "source": [
    "# Compute success rates for a custom test set\n",
    "This notebook is used for computing success rates for a custom test set, where the predictions are performed with the predict.py script."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biotite import structure as struc\n",
    "from biotite.structure.io import pdb\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from AF2Dock.utils.dockq import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f307d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input csv containing id for each target\n",
    "input_csv = \"\"\n",
    "predict_targets = pd.read_csv(input_csv)\n",
    "\n",
    "expt_name = \"\"\n",
    "\n",
    "num_targets = 1\n",
    "num_samples = 40\n",
    "num_samples_per_run = 40\n",
    "\n",
    "# ground truth structure directory\n",
    "gt_pdb_dir = \"\"\n",
    "# input structure directory. input strucutres need to have the same chain names as the ground truth structures\n",
    "input_pdb_dir = \"\"\n",
    "# results directory of AF2Dock predictions\n",
    "results_dir = \"\"\n",
    "\n",
    "# bootstrap parameters\n",
    "num_bootstrap = 10000\n",
    "confidence_metrics_to_rank = ['ipTM_af']\n",
    "confidence_metrics_assending = {'ipTM_af':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4a168",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "metrics_chain = []\n",
    "metrics_global = []\n",
    "for idx in tqdm(range(num_targets)):\n",
    "    data_id = predict_targets.iloc[idx][\"id\"]\n",
    "    gt_pdb_path = f'{gt_pdb_dir}/{data_id}.pdb'\n",
    "    input_rec_pdb_path = f'{input_pdb_dir}/{data_id}_r_holo.pdb'\n",
    "    for sample_idx in range(num_samples):\n",
    "        # find best chain mapping\n",
    "        info = compute_metrics(model = f'{results_dir}/{idx}_{data_id}/{data_id}_s{sample_idx}_ori_chain.pdb',\n",
    "                                native = gt_pdb_path,\n",
    "                                n_cpu=8)\n",
    "\n",
    "        # merge receptor and ligand chains based on best mapping\n",
    "        best_chain_map = info['best_mapping']\n",
    "        input_rec_ori = pdb.PDBFile.read(input_rec_pdb_path)\n",
    "        input_rec_ori_atomarray = pdb.get_structure(input_rec_ori, model=1)\n",
    "        input_rec_ori_chains = list(struc.get_chains(input_rec_ori_atomarray))\n",
    "        chains_to_merge = {'native': [input_rec_ori_chains, [chain for chain in best_chain_map.keys() if chain not in input_rec_ori_chains]]}\n",
    "        chains_to_merge['model'] = [[best_chain_map[chain] for chain in ori_chains] for ori_chains in chains_to_merge['native']]\n",
    "        \n",
    "        # compute metrics after merging\n",
    "        info = compute_metrics(model = f'{results_dir}/{idx}_{data_id}/{data_id}_s{sample_idx}_ori_chain.pdb',\n",
    "                                native = gt_pdb_path,\n",
    "                                n_cpu=8, chains_to_merge=chains_to_merge)\n",
    "        \n",
    "        for key in info['best_result'].keys():\n",
    "            metrics_i = info['best_result'][key]\n",
    "            metrics_i['Model'] = f'{data_id}_s{sample_idx}'\n",
    "            metrics_i['chain_pair'] = key\n",
    "            metrics_chain.append(metrics_i)\n",
    "        metrics_global_i = {'GlobalDockQ': info['GlobalDockQ']}\n",
    "        metrics_global_i['best_dockq'] = info['best_dockq']\n",
    "        metrics_global_i['best_mapping'] = info['best_mapping_str']\n",
    "        metrics_global_i['Model'] = f'{data_id}_s{sample_idx}'\n",
    "        metrics_global.append(metrics_global_i)\n",
    "\n",
    "metrics_chain = pd.DataFrame(metrics_chain)\n",
    "metrics_global = pd.DataFrame(metrics_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_metrics = []\n",
    "for idx in tqdm(range(num_targets)):\n",
    "    data_id = predict_targets.iloc[idx][\"PDB\"].split('.')[0]\n",
    "    # load ipTM scores from csv files\n",
    "    iptm_all = []\n",
    "    for run_idx in range(num_samples // num_samples_per_run):\n",
    "        starting_idx = run_idx * num_samples_per_run\n",
    "        ending_idx = (run_idx + 1) * num_samples_per_run -1\n",
    "        iptm_run = pd.read_csv(f'{results_dir}/{idx}_{data_id}/{data_id}_s{starting_idx}_{ending_idx}_iptm.csv')\n",
    "        iptm_all.append(iptm_run)\n",
    "    confidence_metrics_i = pd.concat(iptm_all, ignore_index=True)\n",
    "    confidence_metrics_i.rename(columns={'iptm': 'ipTM_af'}, inplace=True)\n",
    "    confidence_metrics_i['Model'] = confidence_metrics_i['sample_idx'].apply(lambda x: f'{data_id}_s{x}')\n",
    "    confidence_metrics.append(confidence_metrics_i)\n",
    "\n",
    "confidence_metrics = pd.concat(confidence_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b85610",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = pd.merge(metrics_chain, confidence_metrics, left_on='Model', right_on='Model', how='left').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics.to_csv(f\"combined_metrics_{expt_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5eef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = pd.read_csv(f\"combined_metrics_{expt_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84057cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count_indi_all = {}\n",
    "for confidence_metric_name in confidence_metrics_to_rank:\n",
    "    success_count_indi_all[f'top1_{confidence_metric_name}'] = {'high': [], 'medium': [], 'acceptable': []}\n",
    "    success_count_indi_all[f'top5_{confidence_metric_name}'] = {'high': [], 'medium': [], 'acceptable': []}\n",
    "success_count_indi_all['oracle'] = {'high': [], 'medium': [], 'acceptable': []}\n",
    "np.random.seed(42)\n",
    "\n",
    "for bs_idx in tqdm(range(num_bootstrap)):\n",
    "    per_sample_rand_idx = np.random.randint(num_samples, size=num_samples * num_targets)\n",
    "    sample_base_idx = np.repeat(np.arange(num_targets) * num_samples, num_samples)\n",
    "    randlist = pd.DataFrame(index=per_sample_rand_idx + sample_base_idx)\n",
    "    combined_metrics_bs_i = combined_metrics.merge(randlist, left_index=True, right_index=True, how='right').reset_index(drop=True)\n",
    "    \n",
    "    oracle_top1 = []\n",
    "    for idx in range(num_targets):\n",
    "        sample_i = combined_metrics_bs_i[idx * num_samples: (idx + 1) * num_samples]\n",
    "        oracle_top1_i  = sample_i.sort_values('DockQ', ascending=False).head(1)\n",
    "        oracle_top1.append(oracle_top1_i)\n",
    "    oracle_top1 = pd.concat(oracle_top1)\n",
    "    \n",
    "    confidence_metric_top1 = {confidence_metric_name: [] for confidence_metric_name in confidence_metrics_to_rank}\n",
    "    confidence_metric_top5 = {confidence_metric_name: [] for confidence_metric_name in confidence_metrics_to_rank}\n",
    "    for idx in range(num_targets):\n",
    "        sample_i = combined_metrics_bs_i.iloc[idx * num_samples: (idx + 1) * num_samples]\n",
    "        for confidence_metric_name in confidence_metrics_to_rank:\n",
    "            confidence_metric_top1_i = sample_i.sort_values(confidence_metric_name, ascending=confidence_metrics_assending[confidence_metric_name]).head(1)\n",
    "            confidence_metric_top1[confidence_metric_name].append(confidence_metric_top1_i)\n",
    "            confidence_metric_top5_i = sample_i.sort_values(confidence_metric_name, ascending=confidence_metrics_assending[confidence_metric_name]).head(5)\n",
    "            confidence_metric_top5[confidence_metric_name].append(confidence_metric_top5_i)\n",
    "    confidence_metric_top1 = {confidence_metric_name: pd.concat(confidence_metric_top1[confidence_metric_name]).reset_index(drop=True) for confidence_metric_name in confidence_metrics_to_rank}\n",
    "    confidence_metric_top5 = {confidence_metric_name: pd.concat(confidence_metric_top5[confidence_metric_name]).reset_index(drop=True) for confidence_metric_name in confidence_metrics_to_rank}\n",
    "    \n",
    "    success_count_indi = {}\n",
    "    for confidence_metric_name in confidence_metrics_to_rank:\n",
    "        top1_high = confidence_metric_top1[confidence_metric_name]['DockQ'] >= 0.8\n",
    "        top1_medium = confidence_metric_top1[confidence_metric_name]['DockQ'] >= 0.49\n",
    "        top1_acceptable = confidence_metric_top1[confidence_metric_name]['DockQ'] >= 0.23\n",
    "        success_count_indi[f'top1_{confidence_metric_name}'] = {'high': top1_high, 'medium': top1_medium, 'acceptable':top1_acceptable}\n",
    "        success_count_indi[f'top5_{confidence_metric_name}'] = {'high': 0, 'medium': 0, 'acceptable':0}\n",
    "        top5_high = []\n",
    "        top5_medium = []\n",
    "        top5_acceptable = []\n",
    "        for idx in range(num_targets):\n",
    "            sample_i = confidence_metric_top5[confidence_metric_name].iloc[idx * 5: (idx + 1) * 5]\n",
    "            top5_high.append((sample_i['DockQ'].max() >= 0.8).any())\n",
    "            top5_medium.append((sample_i['DockQ'].max() >= 0.49).any())\n",
    "            top5_acceptable.append((sample_i['DockQ'].max() >= 0.23).any())\n",
    "        success_count_indi[f'top5_{confidence_metric_name}']['high'] = np.array(top5_high)\n",
    "        success_count_indi[f'top5_{confidence_metric_name}']['medium'] = np.array(top5_medium)\n",
    "        success_count_indi[f'top5_{confidence_metric_name}']['acceptable'] = np.array(top5_acceptable)\n",
    "    oracle_high = oracle_top1['DockQ'] >= 0.8\n",
    "    oracle_medium = oracle_top1['DockQ'] >= 0.49\n",
    "    oracle_acceptable = oracle_top1['DockQ'] >= 0.23\n",
    "    success_count_indi['oracle'] = {'high': oracle_high, 'medium': oracle_medium, 'acceptable': oracle_acceptable}\n",
    "\n",
    "    for key in success_count_indi.keys():\n",
    "        for subkey in success_count_indi[key].keys():\n",
    "            success_count_indi_all[key][subkey].append(success_count_indi[key][subkey])\n",
    "    \n",
    "for key in success_count_indi_all.keys():\n",
    "    for subkey in success_count_indi_all[key].keys():\n",
    "        success_count_indi_all[key][subkey] = np.array(success_count_indi_all[key][subkey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebeaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate_CI95_mean = {}\n",
    "success_rate_CI95_err = {}\n",
    "for key in success_count_indi_all.keys():\n",
    "    success_rate_CI95_mean[key] = {}\n",
    "    success_rate_CI95_err[key] = {}\n",
    "    for subkey in success_count_indi_all[key].keys():\n",
    "        success_count = success_count_indi_all[key][subkey].sum(axis=1)\n",
    "        CI95 = [np.percentile(success_count, 2.5),np.percentile(success_count, 97.5)]\n",
    "        CI95_mean = np.mean(success_count)\n",
    "        CI95_err = np.array([CI95[0], CI95[1]])\n",
    "        success_rate_CI95_mean[key][subkey] = CI95_mean / num_targets\n",
    "        success_rate_CI95_err[key][subkey] = CI95_err / num_targets\n",
    "\n",
    "success_rate_CI95_mean = pd.DataFrame(success_rate_CI95_mean)\n",
    "success_rate_CI95_err = pd.DataFrame(success_rate_CI95_err)\n",
    "success_rate_CI95 = {'mean': success_rate_CI95_mean, 'err': success_rate_CI95_err}\n",
    "\n",
    "with open(f'success_rate_CI95_10000_{expt_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(success_rate_CI95, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2b109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'success_rate_CI95_10000_{expt_name}.pkl', 'rb') as f:\n",
    "    success_rate_CI95 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b60d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(3.0, 3.5))\n",
    "\n",
    "combined_results = {stat:{\n",
    "    \"AF2Dock\":{\n",
    "        'top1' : success_rate_CI95[stat]['top1_ipTM_af'],\n",
    "        'top5' : success_rate_CI95[stat]['top5_ipTM_af'],\n",
    "        'oracle' : success_rate_CI95[stat]['oracle']\n",
    "    }} for stat in ['mean', 'err']\n",
    "}\n",
    "\n",
    "cate_larger = ['AF2Dock']\n",
    "metrics = ['top1', 'top5', 'oracle']\n",
    "n_groups = len(cate_larger)\n",
    "n_metrics = len(metrics)\n",
    "\n",
    "# Set up positions for grouped bars\n",
    "bar_width = 0.2  # Width of individual bars\n",
    "bar_spacing = 0.05  # Space between bars within a group\n",
    "group_spacing = 0.2  # Space between groups\n",
    "group_width = n_metrics * bar_width + (n_metrics - 1) * bar_spacing\n",
    "group_positions = np.arange(n_groups) * (group_width + group_spacing)\n",
    "\n",
    "colors = {\n",
    "    'acceptable': '#F3C6AC',\n",
    "    'medium': '#E8674C',\n",
    "    'high': '#761314'\n",
    "}\n",
    "\n",
    "# Create grouped bar data\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_positions = group_positions + i * (bar_width + bar_spacing)\n",
    "    metric_positions_error_bar = np.array([pos + i * (bar_width + bar_spacing) for j, pos in enumerate(group_positions)])\n",
    "\n",
    "    \n",
    "    accu_counts = {\n",
    "        'high': [combined_results['mean'][cat][metric]['high'] * 100 for cat in cate_larger],\n",
    "        'medium': [combined_results['mean'][cat][metric]['medium'] * 100 for cat in cate_larger],\n",
    "        'acceptable': [combined_results['mean'][cat][metric]['acceptable'] * 100 for cat in cate_larger]\n",
    "    }\n",
    "    accu_err_center = {\n",
    "        'high': [np.mean(combined_results['err'][cat][metric]['high']) * 100 for cat in cate_larger],\n",
    "        'medium': [np.mean(combined_results['err'][cat][metric]['medium']) * 100 for cat in cate_larger],\n",
    "        'acceptable': [np.mean(combined_results['err'][cat][metric]['acceptable']) * 100 for cat in cate_larger]\n",
    "    }\n",
    "    accu_err_err = {\n",
    "        'high': [(combined_results['err'][cat][metric]['high'][1] - combined_results['err'][cat][metric]['high'][0]) / 2 * 100 for cat in cate_larger],\n",
    "        'medium': [(combined_results['err'][cat][metric]['medium'][1] - combined_results['err'][cat][metric]['medium'][0]) / 2 * 100 for cat in cate_larger],\n",
    "        'acceptable': [(combined_results['err'][cat][metric]['acceptable'][1] - combined_results['err'][cat][metric]['acceptable'][0]) / 2 * 100 for cat in cate_larger]\n",
    "    }\n",
    "    \n",
    "    previous_heights = [None] * len(cate_larger)  # Initialize previous values for each metric\n",
    "    z_orders = {'high': 3, 'medium': 2, 'acceptable': 1}\n",
    "    ebar_pos_adjust = {'high': bar_width / 2, 'medium': 0, 'acceptable': - bar_width / 2}\n",
    "    for accu_cate in ['high', 'medium', 'acceptable']:\n",
    "        if i == 0:\n",
    "            p = ax.bar(metric_positions, accu_counts[accu_cate], bar_width, capsize=2.0, \n",
    "                    label=accu_cate, color=colors[accu_cate], zorder=z_orders[accu_cate])\n",
    "        else:\n",
    "            p = ax.bar(metric_positions, accu_counts[accu_cate], bar_width, capsize=2.0,\n",
    "                    color=colors[accu_cate], zorder=z_orders[accu_cate])\n",
    "        ebar = ax.errorbar(metric_positions_error_bar + ebar_pos_adjust[accu_cate], accu_err_center[accu_cate], yerr=accu_err_err[accu_cate],\n",
    "                        fmt=\"none\", color=\"gray\", capsize=2.0)\n",
    "        [bar.set_zorder(5) for bar in ebar.lines[1]]\n",
    "        [cap.set_zorder(5) for cap in ebar.lines[2]]\n",
    "\n",
    "        # Add percentage labels on top of bars\n",
    "        for j, (pos, val, previous_height) in enumerate(zip(metric_positions, accu_counts[accu_cate], previous_heights)):\n",
    "            if previous_height is not None and val < previous_height + 2:\n",
    "                height = previous_height + 2\n",
    "            else:\n",
    "                height = val\n",
    "            previous_heights[j] = height  # Update previous value for next iteration\n",
    "            ax.text(pos, height, f'{val:.1f}%', \n",
    "                   ha='center', va='bottom', fontsize=7, rotation=0)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_ylabel('Success Rate (%)')\n",
    "\n",
    "# Add reference lines\n",
    "# Calculate the actual span of all bars\n",
    "first_bar_start = group_positions[0] - bar_width/2\n",
    "last_bar_end = group_positions[-1] + (n_metrics - 1) * (bar_width + bar_spacing) + bar_width/2\n",
    "\n",
    "ax.set_xlim([first_bar_start - 0.2, last_bar_end + 0.2])\n",
    "\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks([])\n",
    "\n",
    "minor_ticks = []\n",
    "minor_labels = []\n",
    "for i, group_pos in enumerate(group_positions):\n",
    "    for j, metric in enumerate(metrics):\n",
    "        minor_ticks.append(group_pos + j * (bar_width + bar_spacing))\n",
    "        minor_labels.append(metric)\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(minor_ticks)\n",
    "ax2.set_xticklabels(minor_labels, rotation=0, ha='center', fontsize=8)\n",
    "ax2.tick_params(axis='x', which='major', top=False, bottom=True, labeltop=False, labelbottom=True, pad=5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python31017jvsc74a57bd00a982d0b7f500a869263a14a2b591da1693216e7763c6d5dadaced094e8812b2",
   "display_name": "Python 3.10.17 64-bit ('AF2Dock': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}